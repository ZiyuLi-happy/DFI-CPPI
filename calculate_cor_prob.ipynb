{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92e4ac1-2784-4f62-841f-9edd462b6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "import os \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dcba6e9-72f5-4886-89c4-4f77d8422b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目录已存在: 0.processed data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LSCC_combined_proteomics_processed.csv',\n",
       " 'LSCC_control_proteomics_processed.csv',\n",
       " 'LSCC_tumor_proteomics_processed.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set variables\n",
    "dir_prefix = '0.processed data/'\n",
    "base_dir = dir_prefix\n",
    "if not os.path.exists(dir_prefix):\n",
    "    os.makedirs(dir_prefix)\n",
    "    print(f\"已创建目录: {dir_prefix}\")\n",
    "else:\n",
    "    print(f\"目录已存在: {dir_prefix}\")\n",
    "\n",
    "# use glob to get all the csv files \n",
    "# in the folder \n",
    "omics_files = [ch for ch in os.listdir(os.getcwd()+'/'+base_dir) if '.csv' in ch]\n",
    "omics_files = sorted(omics_files)\n",
    "\n",
    "# we don't use the RNA data for healthy samples\n",
    "\n",
    "omics_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57de43f6-53f9-4c8e-b49e-f74d005c56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize_columns(df, col1='prot1', col2='prot2'):\n",
    "    \"\"\"\n",
    "    规范化prot1和prot2列：确保每行中prot1 <= prot2\n",
    "    这样 (A, B) 和 (B, A) 会被视为相同的连接\n",
    "    \"\"\"\n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    # 确保prot1和prot2列存在\n",
    "    if col1 not in df.columns or col2 not in df.columns:\n",
    "        raise ValueError(f\"数据框必须包含 '{col1}' 和 '{col2}' 列\")\n",
    "    \n",
    "    # 创建规范化版本\n",
    "    prot1_norm = np.where(df[col1] <= df[col2], df[col1], df[col2])\n",
    "    prot2_norm = np.where(df[col1] <= df[col2], df[col2], df[col1])\n",
    "    \n",
    "    # 添加规范化列\n",
    "    df_normalized[f'{col1}_norm'] = prot1_norm\n",
    "    df_normalized[f'{col2}_norm'] = prot2_norm\n",
    "    \n",
    "    return df_normalized\n",
    "\n",
    "def merge_preserve_left(left_df, right_df, col1='prot1', col2='prot2'):\n",
    "    \"\"\"\n",
    "    拼接两个数据框，保留左侧数据，并处理prot1和prot2列的顺序问题\n",
    "    \"\"\"\n",
    "    # 规范化两个数据框的prot1和prot2列\n",
    "    left_norm = normalize_columns(left_df, col1, col2)\n",
    "    right_norm = normalize_columns(right_df, col1, col2)\n",
    "    \n",
    "    # 执行左连接合并，基于规范化后的列\n",
    "    merged_df = pd.merge(\n",
    "        left_norm,\n",
    "        right_norm,\n",
    "        left_on=[f'{col1}_norm', f'{col2}_norm'],\n",
    "        right_on=[f'{col1}_norm', f'{col2}_norm'],\n",
    "        how='left',\n",
    "        suffixes=('_left', '_right')\n",
    "    )\n",
    "    \n",
    "    # 删除规范化过程中添加的临时列\n",
    "    merged_df.drop([f'{col1}_norm', f'{col2}_norm'], axis=1, inplace=True)\n",
    "    \n",
    "    # 处理prot1和prot2列 - 保留左侧的原始值\n",
    "    # 首先删除右侧的prot1和prot2列\n",
    "    if f'{col1}_right' in merged_df.columns:\n",
    "        merged_df.drop(f'{col1}_right', axis=1, inplace=True)\n",
    "    if f'{col2}_right' in merged_df.columns:\n",
    "        merged_df.drop(f'{col2}_right', axis=1, inplace=True)\n",
    "    \n",
    "    # 重命名左侧的prot1和prot2列，去掉后缀\n",
    "    if f'{col1}_left' in merged_df.columns:\n",
    "        merged_df.rename(columns={f'{col1}_left': col1}, inplace=True)\n",
    "    if f'{col2}_left' in merged_df.columns:\n",
    "        merged_df.rename(columns={f'{col2}_left': col2}, inplace=True)\n",
    "    \n",
    "    # 处理其他重复列（除了prot1和prot2列）\n",
    "    left_cols = set(left_df.columns)\n",
    "    right_cols = set(right_df.columns)\n",
    "    common_cols = left_cols.intersection(right_cols) - {col1, col2}\n",
    "    \n",
    "    # 对于重复列，优先保留左侧的值\n",
    "    for col in common_cols:\n",
    "        left_col = f'{col}_left'\n",
    "        right_col = f'{col}_right'\n",
    "        \n",
    "        if left_col in merged_df.columns and right_col in merged_df.columns:\n",
    "            # 如果左侧列有值，优先使用左侧列\n",
    "            merged_df[col] = merged_df[left_col].combine_first(merged_df[right_col])\n",
    "            \n",
    "            # 删除临时列\n",
    "            merged_df.drop([left_col, right_col], axis=1, inplace=True)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ab697b-15f2-4a0d-87fd-e94e3c090a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSCC_combined: 保存 8094276 对蛋白质关系\n",
      "LSCC_control: 保存 8094276 对蛋白质关系\n",
      "LSCC_tumor: 保存 8094276 对蛋白质关系\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# 1. 计算 Pearson & Partial correlations\n",
    "##############\n",
    "\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "GO=pd.read_csv('0.Data/Go_similarity.csv', sep=',')\n",
    "min_nr_samples = 30\n",
    "base_dir_out = '1.cor_mat/'\n",
    "os.makedirs(base_dir_out, exist_ok=True)\n",
    "\n",
    "for file_name in omics_files:\n",
    "    cur_cohort = file_name.split('_')[0]+'_'+file_name.split('_')[1]\n",
    "    prot_data = pd.read_csv(base_dir+file_name, sep=',', index_col=0)\n",
    "\n",
    "    if prot_data.shape[1] >= min_nr_samples:  # 至少 30 个样本\n",
    "        prot_data.sort_index(inplace=True)\n",
    "\n",
    "        # X: 样本 × 基因\n",
    "        X = prot_data.T.values\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        genes = prot_data.index\n",
    "\n",
    "        ###### 1. Pearson correlation ######\n",
    "        pearson_mat = np.corrcoef(X, rowvar=False)\n",
    "        pearson_mat = pd.DataFrame(pearson_mat, index=genes, columns=genes)\n",
    "\n",
    "        ###### 2. Partial correlation (LedoitWolf 协方差收缩) ######\n",
    "        try:\n",
    "            lw = LedoitWolf().fit(X)\n",
    "            prec_mat = np.linalg.inv(lw.covariance_)\n",
    "            D = np.diag(1 / np.sqrt(np.diag(prec_mat)))\n",
    "            pcorr_mat = -D @ prec_mat @ D\n",
    "            np.fill_diagonal(pcorr_mat, 1)\n",
    "            pcorr_mat = pd.DataFrame(pcorr_mat, index=genes, columns=genes)\n",
    "        except Exception as e:\n",
    "            print(f\"LedoitWolf failed on {cur_cohort}: {e}\")\n",
    "            pcorr_mat = pd.DataFrame(np.nan, index=genes, columns=genes)\n",
    "\n",
    "        ###### 上三角展开（Series） ######\n",
    "        def stack_upper(mat):\n",
    "            iu = np.triu_indices_from(mat, k=1)\n",
    "            idx = pd.MultiIndex.from_arrays(\n",
    "                [mat.index[iu[0]], mat.columns[iu[1]]],\n",
    "                names=[\"prot1\", \"prot2\"]\n",
    "            )\n",
    "            return pd.Series(mat.values[iu], index=idx)\n",
    "\n",
    "        pearson_list = stack_upper(pearson_mat)\n",
    "        pcorr_list   = stack_upper(pcorr_mat)\n",
    "\n",
    "        # 合并\n",
    "        correl_list = pd.concat([pearson_list, pcorr_list], axis=1)\n",
    "        correl_list.columns = [f\"{cur_cohort}_pearson\", f\"{cur_cohort}_pcorr\"]\n",
    "\n",
    "        # reset_index: 让 prot1/prot2 永远是显式列\n",
    "        correl_list = correl_list.reset_index()\n",
    "        merged_df=merge_preserve_left(correl_list, GO)\n",
    "        file_out = file_name.split('_idfix')[0]+'_corr.obj'\n",
    "        with open(base_dir_out+file_out, \"wb\") as file_handler:\n",
    "            pickle.dump(merged_df.round(6), file_handler)\n",
    "\n",
    "        print(f\"{cur_cohort}: 保存 {merged_df.shape[0]} 对蛋白质关系\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59b2dfb4-11ba-4708-b3ee-99d28d54a462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prot1</th>\n",
       "      <th>prot2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BCL6</th>\n",
       "      <th>HDAC4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDAC4</th>\n",
       "      <th>BCL6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BCL6</th>\n",
       "      <th>HDAC5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDAC5</th>\n",
       "      <th>BCL6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BCL6</th>\n",
       "      <th>HDAC7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QRFPR</th>\n",
       "      <th>HCRTR1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRP4</th>\n",
       "      <th>SOST</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOST</th>\n",
       "      <th>LRP4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCRTR2</th>\n",
       "      <th>QRFPR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QRFPR</th>\n",
       "      <th>HCRTR2</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96046 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(BCL6, HDAC4), (HDAC4, BCL6), (BCL6, HDAC5), (HDAC5, BCL6), (BCL6, HDAC7), (HDAC7, BCL6), (CREBBP, EP300), (CREBBP, KAT2B), (CREBBP, NCOA3), (EP300, CREBBP), (EP300, KAT2B), (EP300, NCOA3), (KAT2B, CREBBP), (KAT2B, EP300), (KAT2B, NCOA3), (NCOA3, CREBBP), (NCOA3, EP300), (NCOA3, KAT2B), (NCAPD2, NCAPG), (NCAPD2, NCAPH), (NCAPD2, SMC2), (NCAPD2, SMC4), (NCAPG, NCAPD2), (NCAPG, NCAPH), (NCAPG, SMC2), (NCAPG, SMC4), (NCAPH, NCAPD2), (NCAPH, NCAPG), (NCAPH, SMC2), (NCAPH, SMC4), (SMC2, NCAPD2), (SMC2, NCAPG), (SMC2, NCAPH), (SMC2, SMC4), (SMC4, NCAPD2), (SMC4, NCAPG), (SMC4, NCAPH), (SMC4, SMC2), (HPS1, HPS4), (HPS4, HPS1), (HPS3, HPS5), (HPS3, HPS6), (HPS5, HPS3), (HPS5, HPS6), (HPS6, HPS3), (HPS6, HPS5), (CDS1, MUS81), (MUS81, CDS1), (CORO2A, GPS2), (CORO2A, HDAC3), (CORO2A, NCOR1), (CORO2A, TBL1X), (CORO2A, TBL1XR1), (GPS2, CORO2A), (GPS2, HDAC3), (GPS2, NCOR1), (GPS2, TBL1X), (GPS2, TBL1XR1), (HDAC3, CORO2A), (HDAC3, GPS2), (HDAC3, NCOR1), (HDAC3, TBL1X), (HDAC3, TBL1XR1), (NCOR1, CORO2A), (NCOR1, GPS2), (NCOR1, HDAC3), (NCOR1, TBL1X), (NCOR1, TBL1XR1), (TBL1X, CORO2A), (TBL1X, GPS2), (TBL1X, HDAC3), (TBL1X, NCOR1), (TBL1X, TBL1XR1), (TBL1XR1, CORO2A), (TBL1XR1, GPS2), (TBL1XR1, HDAC3), (TBL1XR1, NCOR1), (TBL1XR1, TBL1X), (BLOC1S1, BLOC1S2), (BLOC1S1, BLOC1S3), (BLOC1S1, BLOC1S4), (BLOC1S1, BLOC1S5), (BLOC1S1, BLOC1S6), (BLOC1S1, DTNBP1), (BLOC1S1, SNAPIN), (BLOC1S2, BLOC1S1), (BLOC1S2, BLOC1S3), (BLOC1S2, BLOC1S4), (BLOC1S2, BLOC1S5), (BLOC1S2, BLOC1S6), (BLOC1S2, DTNBP1), (BLOC1S2, SNAPIN), (BLOC1S3, BLOC1S1), (BLOC1S3, BLOC1S2), (BLOC1S3, BLOC1S4), (BLOC1S3, BLOC1S5), (BLOC1S3, BLOC1S6), (BLOC1S3, DTNBP1), (BLOC1S3, SNAPIN), (BLOC1S4, BLOC1S1), ...]\n",
       "\n",
       "[96046 rows x 0 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dir_prefix = '0.Data/'\n",
    "# conversion tool for CORUM proteins to the standard gene set we use\n",
    "conversion_df = pd.read_csv(dir_prefix+'[fig1a]_conversion_df_240827.csv')\n",
    "conversion_df.set_index('from_id',inplace=True)\n",
    "\n",
    "# load human interactions from CORUM database\n",
    "#file_name =\n",
    "temp_df = pd.read_csv(dir_prefix+'corum_humanComplexes.txt',sep='\\t')\n",
    "#temp_df = temp_df.loc[temp_df.loc[:,'Organism']=='Human',:]\n",
    "CORUM_complexes = temp_df['subunits_gene_name'].reset_index(drop=True)\n",
    "\n",
    "corum_set = []\n",
    "for complex_nr in range(len(CORUM_complexes)):\n",
    "    # get CORUM complex members and number of parts\n",
    "    cur_compl = CORUM_complexes[complex_nr].split(';')\n",
    "    cur_compl = [x.strip() for x in cur_compl if x]\n",
    "    cur_compl = [x.upper() for x in cur_compl if 'orf' not in x]+[x for x in cur_compl if 'orf' in x]\n",
    "    cur_compl = [x for x in cur_compl if 'NONE' not in x]\n",
    "    cur_compl = [x for x in cur_compl if 'B-RAF' not in x]+['BRAF' for x in cur_compl if 'B-RAF' in x]\n",
    "    \n",
    "    # store all combinations of protein pairs in the complex\n",
    "    n_parts = len(cur_compl)\n",
    "    for i in range(n_parts):\n",
    "        if ('variant' not in cur_compl[i]) & ('None' not in cur_compl[i]):\n",
    "            for j in range(n_parts):\n",
    "                if ('variant' not in cur_compl[j]) & ('None' not in cur_compl[j]):\n",
    "                    if i != j:\n",
    "                        corum_set.append([cur_compl[i],cur_compl[j]])\n",
    "\n",
    "\n",
    "corum_db = (pd.DataFrame(corum_set))\n",
    "corum_db=corum_db.drop_duplicates()\n",
    "corum_db.columns=['prot1','prot2']\n",
    "\n",
    "corum_db = corum_db.loc[corum_db['prot1'].isin(conversion_df.index) & corum_db['prot2'].isin(conversion_df.index)]\n",
    "corum_db.loc[:,'prot1']=list(conversion_df.loc[corum_db['prot1'],'to_id'])\n",
    "corum_db.loc[:,'prot2']=list(conversion_df.loc[corum_db['prot2'],'to_id'])\n",
    "corum_db.set_index(['prot1','prot2'],inplace=True)\n",
    "corum_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91437d68-7216-4ac7-9458-8c76d44d1752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LSCC_combined_proteomics_processed.csv_corr.obj',\n",
       " 'LSCC_control_proteomics_processed.csv_corr.obj',\n",
       " 'LSCC_tumor_proteomics_processed.csv_corr.obj']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set variables\n",
    "base_dir = '1.cor_mat/'\n",
    "\n",
    "# use glob to get all the csv files \n",
    "# in the folder \n",
    "omics_files = [ch for ch in os.listdir(os.getcwd()+'/'+base_dir) if '.obj' in ch]\n",
    "omics_files = sorted(omics_files)\n",
    "\n",
    "omics_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e51a6d0-c9bd-4cb8-98f6-f84f4315e679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSCC_combined_proteomics_processed.csv_corr.obj: 已保存概率 7632795 对蛋白质\n",
      "LSCC_control_proteomics_processed.csv_corr.obj: 已保存概率 7632795 对蛋白质\n",
      "LSCC_tumor_proteomics_processed.csv_corr.obj: 已保存概率 7632795 对蛋白质\n"
     ]
    }
   ],
   "source": [
    "\n",
    "string=pd.read_table(\"9606 link merged.txt\",index_col=[0,1])\n",
    "\n",
    "##############\n",
    "# 2. 训练 Logistic Regression 并输出概率\n",
    "##############\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base_dir_out = '2.probabilities/'\n",
    "os.makedirs(base_dir_out, exist_ok=True)\n",
    "\n",
    "for file_name in omics_files:\n",
    "    # load correlation file\n",
    "    with open('1.cor_mat/'+file_name, \"rb\") as file_handler:\n",
    "        cur_correl = pickle.load(file_handler)\n",
    "\n",
    "    # 设置 index = (prot1, prot2)，方便后续匹配 corum_db\n",
    "    cur_correl = cur_correl.set_index(['prot1','prot2'])\n",
    "\n",
    "    # annotate with CORUM\n",
    "    cur_correl.loc[~cur_correl.index.isin(string.index), 'corum'] = 0\n",
    "    cur_correl.loc[cur_correl.index.isin(corum_db.index), 'corum'] = 1\n",
    "    cur_correl.loc[cur_correl['GO:MF'].isna(), 'GO:MF'] = 0\n",
    "    cur_correl.loc[cur_correl['GO:BP'].isna(), 'GO:BP'] = 0\n",
    "    cur_correl.loc[cur_correl['GO:CC'].isna(), 'GO:CC'] = 0\n",
    "    cur_correl=cur_correl.loc[~cur_correl['corum'].isna(),]\n",
    "    # \n",
    "    X = cur_correl.iloc[:, :5].values   # 前两列是 pearson 和 pcorr\n",
    "    y = cur_correl['corum'].values\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        penalty=None, class_weight='balanced',\n",
    "        fit_intercept=True, solver='newton-cholesky'\n",
    "    ).fit(X, y)\n",
    "\n",
    "    # 预测概率（正类）\n",
    "    cur_prob = pd.DataFrame(\n",
    "        clf.predict_proba(X)[:, 1],\n",
    "        index=cur_correl.index,\n",
    "        columns=[file_name.split('_idfix')[0]]\n",
    "    )\n",
    "\n",
    "    file_out = file_name.split('_idfix')[0]+'_prob.obj'\n",
    "    with open(base_dir_out+file_out, \"wb\") as file_handler:\n",
    "        pickle.dump(cur_prob.round(7), file_handler)\n",
    "\n",
    "    print(f\"{file_name}: 已保存概率 {cur_prob.shape[0]} 对蛋白质\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2276803-7f0f-49f6-8244-ca82d35213e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[LSCC_combined] XGBoost 已完成 | AUC=0.964 | AP=0.921\n",
      "最佳参数: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 4, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "模型已保存到: 3.xgb_probabilities/LSCC_combined_proteomics_processed.csv_corr.obj_model.pkl\n",
      "4.0\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[LSCC_control] XGBoost 已完成 | AUC=0.964 | AP=0.912\n",
      "最佳参数: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 4, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "模型已保存到: 3.xgb_probabilities/LSCC_control_proteomics_processed.csv_corr.obj_model.pkl\n",
      "4.0\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[LSCC_tumor] XGBoost 已完成 | AUC=0.961 | AP=0.911\n",
      "最佳参数: {'subsample': 0.6, 'reg_lambda': 3, 'reg_alpha': 1, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 2, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "模型已保存到: 3.xgb_probabilities/LSCC_tumor_proteomics_processed.csv_corr.obj_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "##############\n",
    "# 3. XGBoost版本训练并输出概率（完整版）\n",
    "##############\n",
    "\n",
    "# ==== 路径设置 ====\n",
    "string = pd.read_table(\"9606 link merged.txt\", index_col=[0, 1])\n",
    "xgb_dir_out = '3.xgb_probabilities/'\n",
    "xgb_eval_dir_out = '3.xgb_evaluation_plots/'\n",
    "os.makedirs(xgb_dir_out, exist_ok=True)\n",
    "os.makedirs(xgb_eval_dir_out, exist_ok=True)\n",
    "\n",
    "\n",
    "# ==== 采样函数 ====\n",
    "def sample_negatives_with_na(df, ratio=2.0, label_col=\"corum\"):\n",
    "    positives = df[df[label_col] == 1]\n",
    "    negatives = df[df[label_col] == 0]\n",
    "    na_samples = df[df[label_col].isna()]\n",
    "\n",
    "    n_pos = len(positives)\n",
    "    n_neg_sample = int(min(len(negatives), ratio * n_pos))\n",
    "\n",
    "    neg_sample = negatives.sample(n=n_neg_sample, random_state=42)\n",
    "    return pd.concat([positives, neg_sample])\n",
    "\n",
    "\n",
    "# ==== 模型训练 + 超参搜索 ====\n",
    "def train_xgb_model(X_train, y_train):\n",
    "    param_dist = {\n",
    "        \"max_depth\": [1,2,3,4],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"subsample\": [0.6, 0.8],\n",
    "        \"colsample_bytree\": [0.6, 0.8],\n",
    "        \"n_estimators\": [100, 300, 500],\n",
    "        \"min_child_weight\": [1, 3, 5],\n",
    "        'reg_alpha': [0,1, 3, 5],\n",
    "        'reg_lambda': [1,3,2]\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='aucpr',\n",
    "        tree_method='hist',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=8\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,\n",
    "        scoring=\"average_precision\",\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    return search.best_estimator_, search.best_params_\n",
    "\n",
    "\n",
    "# ==== 主循环 ====\n",
    "all_predictions = []   # 保存所有文件的预测结果\n",
    "\n",
    "for file_name in omics_files:\n",
    "    # 1. 加载特征数据\n",
    "    with open(f'1.cor_mat/{file_name}', \"rb\") as file_handler:\n",
    "        cur_correl = pickle.load(file_handler)\n",
    "\n",
    "    cur_correl = cur_correl.set_index(['prot1', 'prot2'])\n",
    "\n",
    "    # 2. 标注 CORUM\n",
    "    cur_correl.loc[cur_correl.index.isin(corum_db.index), 'corum'] = 1\n",
    "    cur_correl.loc[~cur_correl.index.isin(string.index), 'corum'] = 0\n",
    "    # NA 保留\n",
    "\n",
    "    for go_term in ['GO:MF', 'GO:BP', 'GO:CC']:\n",
    "        cur_correl.loc[cur_correl[go_term].isna(), go_term] = 0\n",
    "\n",
    "    # 3. 采样（训练用）\n",
    "    cur_sampled = sample_negatives_with_na(cur_correl, ratio=4)\n",
    "\n",
    "    # 4. 提取特征和标签 (仅 0/1)\n",
    "    X = cur_sampled.iloc[:, :5]\n",
    "    y = cur_sampled['corum']\n",
    "    print(sum(y == 0) / sum(y == 1))\n",
    "    # 5. 拆分训练/测试\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # 6. 超参搜索 + 模型训练\n",
    "    best_model, best_params = train_xgb_model(X_train, y_train)\n",
    "\n",
    "    # 7. 在测试集上评估\n",
    "    y_test_pred = best_model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_test_pred)\n",
    "    avg_precision = average_precision_score(y_test, y_test_pred)\n",
    "\n",
    "    # 绘制评估图\n",
    "    prefix = file_name.split('_idfix')[0]\n",
    "    prefix = prefix.split(\"_pro\")[0]\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    ax1.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "             label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax1.set_xlim([0.0, 1.0]); ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate'); ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title(f'{prefix} - XGBoost ROC Curve'); ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    ax2.plot(recall, precision, color='blue', lw=2,\n",
    "             label=f'PR curve (AP = {avg_precision:.3f})')\n",
    "    ax2.set_xlim([0.0, 1.0]); ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('Recall'); ax2.set_ylabel('Precision')\n",
    "    ax2.set_title(f'{prefix} - XGBoost Precision-Recall Curve')\n",
    "    ax2.legend(loc=\"lower left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(xgb_eval_dir_out, f\"{prefix}_xgb_eval.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[{prefix}] XGBoost 已完成 | AUC={roc_auc:.3f} | AP={avg_precision:.3f}\")\n",
    "    print(f\"最佳参数: {best_params}\")\n",
    "\n",
    "    # 8. 对整体数据做预测（包括 NA 和未采样负样本）\n",
    "    X_all = cur_correl.iloc[:, :5]\n",
    "    y_all_pred = best_model.predict_proba(X_all)[:, 1]\n",
    "\n",
    "    cur_all_prob = pd.DataFrame({\n",
    "        \"prot1\": X_all.index.get_level_values(0),\n",
    "        \"prot2\": X_all.index.get_level_values(1),\n",
    "        \"probability\": y_all_pred\n",
    "    }).set_index([\"prot1\", \"prot2\"])\n",
    "    prefix = file_name.split('_idfix')[0]\n",
    "    cur_all_prob.to_csv(os.path.join(xgb_dir_out, f\"{prefix}_predictions.csv\"))\n",
    "    #保存模型\n",
    "    \n",
    "    model_filename = os.path.join(xgb_dir_out, f\"{prefix}_model.pkl\")\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(best_model, model_file)\n",
    "    print(f\"模型已保存到: {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fc670ba-e69d-4bcd-aa71-22801351a15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1248.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6240/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98c2c062-6ef5-4e76-b2f7-02fa352f9a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive/Negative ratio: 1248:4992 = 1:4.0\n",
      "\n",
      "Training feature set: Correlation\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation] AUC=0.752 | AP=0.556\n",
      "Best parameters: {'subsample': 0.6, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 2, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial] AUC=0.776 | AP=0.607\n",
      "Best parameters: {'subsample': 0.6, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 2, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial+GO:MF\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+GO:MF] AUC=0.878 | AP=0.749\n",
      "Best parameters: {'subsample': 0.6, 'reg_lambda': 1, 'reg_alpha': 3, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial+GO:BP\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+GO:BP] AUC=0.936 | AP=0.867\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 2, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 2, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial+GO:CC\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+GO:CC] AUC=0.909 | AP=0.827\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 5, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial+All GO\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+All GO] AUC=0.964 | AP=0.921\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 4, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "Model saved to: 3.xgb_probabilities/LSCC_combined_model.pkl\n",
      "[LSCC_combined] Evaluation of all feature sets completed\n",
      "Positive/Negative ratio: 1248:4992 = 1:4.0\n",
      "\n",
      "Training feature set: Correlation\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation] AUC=0.697 | AP=0.451\n",
      "Best parameters: {'subsample': 0.6, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 2, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial] AUC=0.712 | AP=0.492\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 2, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 2, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial+GO:MF\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+GO:MF] AUC=0.838 | AP=0.670\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 4, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "\n",
      "Training feature set: Correlation+Partial+GO:BP\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+GO:BP] AUC=0.933 | AP=0.841\n",
      "Best parameters: {'subsample': 0.6, 'reg_lambda': 3, 'reg_alpha': 5, 'n_estimators': 500, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial+GO:CC\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+GO:CC] AUC=0.907 | AP=0.803\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 5, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial+All GO\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+All GO] AUC=0.964 | AP=0.912\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 4, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "Model saved to: 3.xgb_probabilities/LSCC_control_model.pkl\n",
      "[LSCC_control] Evaluation of all feature sets completed\n",
      "Positive/Negative ratio: 1248:4992 = 1:4.0\n",
      "\n",
      "Training feature set: Correlation\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation] AUC=0.744 | AP=0.597\n",
      "Best parameters: {'subsample': 0.6, 'reg_lambda': 2, 'reg_alpha': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "\n",
      "Training feature set: Correlation+Partial\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial] AUC=0.753 | AP=0.604\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 3, 'reg_alpha': 3, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 1, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "\n",
      "Training feature set: Correlation+Partial+GO:MF\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+GO:MF] AUC=0.853 | AP=0.725\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 3, 'reg_alpha': 3, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.1, 'colsample_bytree': 0.6}\n",
      "\n",
      "Training feature set: Correlation+Partial+GO:BP\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+GO:BP] AUC=0.930 | AP=0.859\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 2, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 2, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial+GO:CC\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+GO:CC] AUC=0.907 | AP=0.824\n",
      "Best parameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 5, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "Training feature set: Correlation+Partial+All GO\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[Correlation+Partial+All GO] AUC=0.961 | AP=0.911\n",
      "Best parameters: {'subsample': 0.6, 'reg_lambda': 3, 'reg_alpha': 1, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 2, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "Model saved to: 3.xgb_probabilities/LSCC_tumor_model.pkl\n",
      "[LSCC_tumor] Evaluation of all feature sets completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "##############\n",
    "# 3. XGBoost Model Training with Probability Output (Full Version)\n",
    "##############\n",
    "\n",
    "# ==== Path Settings ====\n",
    "string = pd.read_table(\"9606 link merged.txt\", index_col=[0, 1])\n",
    "xgb_dir_out = '3.xgb_probabilities/'\n",
    "xgb_eval_dir_out = '3.xgb_evaluation_plots/'\n",
    "os.makedirs(xgb_dir_out, exist_ok=True)\n",
    "os.makedirs(xgb_eval_dir_out, exist_ok=True)\n",
    "\n",
    "# ==== Feature Set Definitions ====\n",
    "FEATURE_SETS = {\n",
    "    \"Correlation\": [0],\n",
    "    \"Correlation+Partial\": [0, 1],\n",
    "    \"Correlation+Partial+GO:MF\": [0, 1, 2],\n",
    "    \"Correlation+Partial+GO:BP\": [0, 1, 3],\n",
    "    \"Correlation+Partial+GO:CC\": [0, 1, 4],\n",
    "    \"Correlation+Partial+All GO\": [0, 1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "# ==== Sampling Function ====\n",
    "def sample_negatives_with_na(df, ratio=2.0, label_col=\"corum\"):\n",
    "    positives = df[df[label_col] == 1]\n",
    "    negatives = df[df[label_col] == 0]\n",
    "    na_samples = df[df[label_col].isna()]\n",
    "\n",
    "    n_pos = len(positives)\n",
    "    n_neg_sample = int(min(len(negatives), ratio * n_pos))\n",
    "\n",
    "    neg_sample = negatives.sample(n=n_neg_sample, random_state=42)\n",
    "    return pd.concat([positives, neg_sample])\n",
    "\n",
    "# ==== Model Training + Hyperparameter Tuning ====\n",
    "def train_xgb_model(X_train, y_train):\n",
    "    param_dist = {\n",
    "        \"max_depth\": [1,2,3,4],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"subsample\": [0.6, 0.8],\n",
    "        \"colsample_bytree\": [0.6, 0.8],\n",
    "        \"n_estimators\": [100, 300, 500],\n",
    "        \"min_child_weight\": [1, 3, 5],\n",
    "        'reg_alpha': [0,1, 3, 5],\n",
    "        'reg_lambda': [1,3,2]\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='aucpr',\n",
    "        tree_method='hist',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=8\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,\n",
    "        scoring=\"average_precision\",\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    return search.best_estimator_, search.best_params_\n",
    "\n",
    "# ==== Main Loop ====\n",
    "all_predictions = []   # Save predictions for all files\n",
    "\n",
    "for file_name in omics_files:\n",
    "    # 1. Load feature data\n",
    "    with open(f'1.cor_mat/{file_name}', \"rb\") as file_handler:\n",
    "        cur_correl = pickle.load(file_handler)\n",
    "\n",
    "    cur_correl = cur_correl.set_index(['prot1', 'prot2'])\n",
    "\n",
    "    # 2. Label CORUM\n",
    "    cur_correl.loc[cur_correl.index.isin(corum_db.index), 'corum'] = 1\n",
    "    cur_correl.loc[~cur_correl.index.isin(string.index), 'corum'] = 0\n",
    "    # NA remains\n",
    "\n",
    "    for go_term in ['GO:MF', 'GO:BP', 'GO:CC']:\n",
    "        cur_correl.loc[cur_correl[go_term].isna(), go_term] = 0\n",
    "\n",
    "    # 3. Sampling (for training)\n",
    "    cur_sampled = sample_negatives_with_na(cur_correl, ratio=4)\n",
    "\n",
    "    # 4. Extract features and labels (only 0/1)\n",
    "    X = cur_sampled.iloc[:, :5]\n",
    "    y = cur_sampled['corum']\n",
    "    print(f\"Positive/Negative ratio: {sum(y == 1)}:{sum(y == 0)} = 1:{sum(y == 0)/sum(y == 1):.1f}\")\n",
    "    \n",
    "    # 5. Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Store results for each feature set\n",
    "    results = {}\n",
    "    \n",
    "    # 6. Train and evaluate each feature set\n",
    "    for feature_name, feature_indices in FEATURE_SETS.items():\n",
    "        print(f\"\\nTraining feature set: {feature_name}\")\n",
    "        \n",
    "        # Select feature subset\n",
    "        X_train_sub = X_train.iloc[:, feature_indices]\n",
    "        X_test_sub = X_test.iloc[:, feature_indices]\n",
    "        \n",
    "        # Hyperparameter tuning + model training\n",
    "        best_model, best_params = train_xgb_model(X_train_sub, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_test_pred = best_model.predict_proba(X_test_sub)[:, 1]\n",
    "        \n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_test_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Calculate PR curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_test_pred)\n",
    "        avg_precision = average_precision_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[feature_name] = {\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'roc_auc': roc_auc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'avg_precision': avg_precision,\n",
    "            'model': best_model,\n",
    "            'params': best_params\n",
    "        }\n",
    "        \n",
    "        print(f\"[{feature_name}] AUC={roc_auc:.3f} | AP={avg_precision:.3f}\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "    # 7. Create comparison plots\n",
    "    prefix = file_name.split('_idfix')[0]\n",
    "    prefix = prefix.split(\"_pro\")[0]\n",
    "    \n",
    "    # Create plots\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # ROC curve\n",
    "    plt.subplot(2, 1, 1)\n",
    "    colors = cycle(['blue', 'green', 'red', 'purple', 'orange', 'brown'])\n",
    "    for i, (feature_name, result) in enumerate(results.items()):\n",
    "        plt.plot(result['fpr'], result['tpr'], color=next(colors), lw=2,\n",
    "                 label=f'{feature_name} (AUC = {result[\"roc_auc\"]:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{prefix} - ROC Curve Comparison')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # PR curve\n",
    "    plt.subplot(2, 1, 2)\n",
    "    colors = cycle(['blue', 'green', 'red', 'purple', 'orange', 'brown'])\n",
    "    for i, (feature_name, result) in enumerate(results.items()):\n",
    "        plt.plot(result['recall'], result['precision'], color=next(colors), lw=2,\n",
    "                 label=f'{feature_name} (AP = {result[\"avg_precision\"]:.3f})')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{prefix} - Precision-Recall Curve Comparison')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(xgb_eval_dir_out, f\"{prefix}_feature_comparison.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 8. Feature importance plot (for full feature set)\n",
    "    full_model = results['Correlation+Partial+All GO']['model']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    xgb.plot_importance(full_model, max_num_features=10)\n",
    "    plt.title(f'{prefix} - Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(xgb_eval_dir_out, f\"{prefix}_feature_importance.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 9. Performance metric comparison plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # AUC comparison\n",
    "    feature_names = list(results.keys())\n",
    "    auc_scores = [results[name]['roc_auc'] for name in feature_names]\n",
    "    ap_scores = [results[name]['avg_precision'] for name in feature_names]\n",
    "    \n",
    "    ax1.bar(feature_names, auc_scores, color='skyblue')\n",
    "    ax1.set_title('AUC Comparison')\n",
    "    ax1.set_ylabel('AUC')\n",
    "    ax1.set_ylim([0.5, 1.0])\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # AP comparison\n",
    "    ax2.bar(feature_names, ap_scores, color='lightgreen')\n",
    "    ax2.set_title('Average Precision (AP) Comparison')\n",
    "    ax2.set_ylabel('Average Precision')\n",
    "    ax2.set_ylim([0.0, 1.0])\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(xgb_eval_dir_out, f\"{prefix}_performance_comparison.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 10. Predict on all data (using full feature model)\n",
    "    X_all = cur_correl.iloc[:, :5]\n",
    "    y_all_pred = full_model.predict_proba(X_all)[:, 1]\n",
    "\n",
    "    cur_all_prob = pd.DataFrame({\n",
    "        \"prot1\": X_all.index.get_level_values(0),\n",
    "        \"prot2\": X_all.index.get_level_values(1),\n",
    "        \"probability\": y_all_pred\n",
    "    }).set_index([\"prot1\", \"prot2\"])\n",
    "    \n",
    "    cur_all_prob.to_csv(os.path.join(xgb_dir_out, f\"{prefix}_predictions.csv\"))\n",
    "    \n",
    "    # 11. Save full feature model\n",
    "    model_filename = os.path.join(xgb_dir_out, f\"{prefix}_model.pkl\")\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(full_model, model_file)\n",
    "    print(f\"Model saved to: {model_filename}\")\n",
    "\n",
    "    print(f\"[{prefix}] Evaluation of all feature sets completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13658a12-2c33-4f68-b550-e42e9d0dce02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591cf9f-ee94-4931-a29e-15a5918615b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
